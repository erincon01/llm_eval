Data source: sql-server

Performance Report per model:

| model                                  |   queries_executed |   mean_sql_time |   mean_llm_time |   stdev_llm_time |   mean_tokens |   mean_datasets_equality |   mean_cost_EUR |
|:---------------------------------------|-------------------:|----------------:|----------------:|-----------------:|--------------:|-------------------------:|----------------:|
| DeepSeek-V3-0324                       |                 20 |            2.68 |            4.38 |             2.48 |       2041.3  |                     0.77 |        0.002916 |
| Llama-3.3-70B-Instruct                 |                 20 |            3.37 |           28.38 |            42.63 |       1975.05 |                     0.67 |        0.011145 |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |                 20 |            5.31 |            2.16 |             0.85 |       1969.35 |                     0.7  |        0.000321 |
| Llama-4-Scout-17B-16E-Instruct         |                 20 |            2.62 |            2.59 |             1.19 |       1995.75 |                     0.65 |        0.000517 |
| Mistral-Large-2411                     |                 20 |            3.5  |            5.58 |             3.12 |       2641.15 |                     0.72 |        0.001875 |
| Phi-3-medium-128k-instruct             |                 20 |            0    |            8.81 |             9.16 |        427.75 |                     0    |        0.000229 |
| claude-3-5-haiku-20241022              |                 20 |            3.22 |            4.04 |             1.62 |       2473.35 |                     0.7  |        0.00278  |
| claude-3-5-sonnet-20241022             |                 20 |            3.3  |            4.66 |             0.98 |       2442.7  |                     0.78 |        0.009965 |
| claude-3-7-sonnet-20250219             |                 20 |            2.62 |            4.12 |             1.5  |       2449.95 |                     0.72 |        0.010074 |
| claude-sonnet-4-20250514               |                 20 |            2.76 |            5.48 |             1.83 |       2456.2  |                     0.75 |        0.010168 |
| gpt-4.1                                |                 20 |            2.6  |            1.84 |             0.86 |       1982.7  |                     0.85 |        0.005108 |
| gpt-4.1-mini                           |                 20 |            2.77 |            2.1  |             0.77 |       1981.35 |                     0.8  |        0.00102  |
| gpt-4o                                 |                 20 |            3.11 |            2.62 |             1.05 |       1966.85 |                     0.9  |        0.006227 |
| gpt-4o-mini                            |                 20 |            2.48 |            2.16 |             0.73 |       1967.9  |                     0.6  |        0.000374 |
| grok-3                                 |                 20 |            2.9  |            3.08 |             1.02 |       1941.3  |                     0.85 |        0.007913 |
| grok-3-mini                            |                 20 |            2.95 |           16.49 |             9.13 |       4384.5  |                     0.8  |        0.000662 |

Performance Report per query:

|   question |   mean_llm_time |   stdev_llm_time |   mean_rows_equality |   mean_columns_equality |   mean_datasets_equality |
|-----------:|----------------:|-----------------:|---------------------:|------------------------:|-------------------------:|
|          1 |            3.38 |             2.14 |                 0.94 |                    0.94 |                     0.94 |
|          2 |            3.24 |             2.69 |                 0.94 |                    0.94 |                     0.94 |
|          3 |            2.54 |             1.19 |                 0.94 |                    0.94 |                     0.94 |
|          4 |            5.76 |             3.77 |                 0.67 |                    0.88 |                     0.64 |
|          5 |            6.06 |             7.18 |                 0.56 |                    0.55 |                     0    |
|          6 |            3.95 |             1.96 |                 0.94 |                    0.88 |                     0.75 |
|          7 |            3.68 |             3.21 |                 0.94 |                    0.94 |                     0.94 |
|          8 |            3.65 |             2.13 |                 0.94 |                    0.94 |                     0.94 |
|          9 |           13.04 |            33.61 |                 0.81 |                    0.88 |                     0.84 |
|         10 |            4.17 |             4.14 |                 0.94 |                    0.94 |                     0.94 |
|         11 |            8.6  |            23.28 |                 0.94 |                    0.75 |                     0.75 |
|         12 |            2.53 |             1.75 |                 0.94 |                    0.94 |                     0.94 |
|         13 |           11.31 |            27.9  |                 0.94 |                    0.94 |                     0.94 |
|         14 |            4.24 |             3.84 |                 0.88 |                    0.94 |                     0.88 |
|         15 |            7.21 |             6.5  |                 0.88 |                    0.82 |                     0.81 |
|         16 |           12.37 |            20.43 |                 0.57 |                    0.53 |                     0.38 |
|         17 |            9.27 |            11.36 |                 0.67 |                    0.65 |                     0    |
|         18 |            5.78 |             7    |                 0.81 |                    0.72 |                     0.44 |
|         19 |            5.42 |             3.08 |                 0.56 |                    0.53 |                     0.44 |
|         20 |            6.96 |             8.79 |                 0.88 |                    0.79 |                     0.62 |

Best models based on average LLM time:

| model                                  |   mean_llm_time |
|:---------------------------------------|----------------:|
| gpt-4.1                                |            1.84 |
| gpt-4.1-mini                           |            2.1  |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |            2.16 |
| gpt-4o-mini                            |            2.16 |
| Llama-4-Scout-17B-16E-Instruct         |            2.59 |
| gpt-4o                                 |            2.62 |
| grok-3                                 |            3.08 |
| claude-3-5-haiku-20241022              |            4.04 |
| claude-3-7-sonnet-20250219             |            4.12 |
| DeepSeek-V3-0324                       |            4.38 |
| claude-3-5-sonnet-20241022             |            4.66 |
| claude-sonnet-4-20250514               |            5.48 |
| Mistral-Large-2411                     |            5.58 |
| Phi-3-medium-128k-instruct             |            8.81 |
| grok-3-mini                            |           16.49 |
| Llama-3.3-70B-Instruct                 |           28.38 |

Best models based on mean token cost:

| model                                  |   mean_cost_EUR |
|:---------------------------------------|----------------:|
| Phi-3-medium-128k-instruct             |        0.000229 |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |        0.000321 |
| gpt-4o-mini                            |        0.000374 |
| Llama-4-Scout-17B-16E-Instruct         |        0.000517 |
| grok-3-mini                            |        0.000662 |
| gpt-4.1-mini                           |        0.00102  |
| Mistral-Large-2411                     |        0.001875 |
| claude-3-5-haiku-20241022              |        0.00278  |
| DeepSeek-V3-0324                       |        0.002916 |
| gpt-4.1                                |        0.005108 |
| gpt-4o                                 |        0.006227 |
| grok-3                                 |        0.007913 |
| claude-3-5-sonnet-20241022             |        0.009965 |
| claude-3-7-sonnet-20250219             |        0.010074 |
| claude-sonnet-4-20250514               |        0.010168 |
| Llama-3.3-70B-Instruct                 |        0.011145 |

Best models based on average datasets equality:

| model                                  |   mean_datasets_equality |
|:---------------------------------------|-------------------------:|
| gpt-4o                                 |                     0.9  |
| gpt-4.1                                |                     0.85 |
| grok-3                                 |                     0.85 |
| gpt-4.1-mini                           |                     0.8  |
| grok-3-mini                            |                     0.8  |
| claude-3-5-sonnet-20241022             |                     0.78 |
| DeepSeek-V3-0324                       |                     0.77 |
| claude-sonnet-4-20250514               |                     0.75 |
| claude-3-7-sonnet-20250219             |                     0.72 |
| Mistral-Large-2411                     |                     0.72 |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |                     0.7  |
| claude-3-5-haiku-20241022              |                     0.7  |
| Llama-3.3-70B-Instruct                 |                     0.67 |
| Llama-4-Scout-17B-16E-Instruct         |                     0.65 |
| gpt-4o-mini                            |                     0.6  |
| Phi-3-medium-128k-instruct             |                     0    |


Ranking of the models based on the total cost, LLM time and source rows equality:

| model                                  | rank_quality   | rank_time   | rank_price   |
|:---------------------------------------|:---------------|:------------|:-------------|
| gpt-4o                                 | 1 (0.9)        | 6 (2.62)    |              |
| gpt-4.1                                | 2 (0.85)       | 1 (1.84)    |              |
| grok-3                                 | 2 (0.85)       | 7 (3.08)    |              |
| gpt-4.1-mini                           | 4 (0.8)        | 2 (2.1)     | 6 (0.00102)  |
| grok-3-mini                            | 4 (0.8)        |             | 5 (0.000662) |
| claude-3-5-sonnet-20241022             | 6 (0.78)       |             |              |
| DeepSeek-V3-0324                       | 7 (0.77)       |             |              |
| claude-sonnet-4-20250514               | 8 (0.75)       |             |              |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |                | 3 (2.16)    | 2 (0.000321) |
| gpt-4o-mini                            |                | 3 (2.16)    | 3 (0.000374) |
| Llama-4-Scout-17B-16E-Instruct         |                | 5 (2.59)    | 4 (0.000517) |
| claude-3-5-haiku-20241022              |                | 8 (4.04)    | 8 (0.00278)  |
| Phi-3-medium-128k-instruct             |                |             | 1 (0.000229) |
| Mistral-Large-2411                     |                |             | 7 (0.001875) |
| Llama-3.3-70B-Instruct                 |                |             |              |
| claude-3-7-sonnet-20250219             |                |             |              |



