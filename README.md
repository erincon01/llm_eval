# llm_eval

# Project Overview

This project is designed to evaluate the performance of various Large Language Models (LLMs) in generating SQL queries based on predefined questions and database schemas. It provides tools for orchestrating evaluations, comparing results, and generating performance reports.

## The process

- Given a set of questions (`01-questions.yaml`), 
- Given a database schema (`02-database_schema.yaml`),
- Given a list of semantic rules (`03-semantic-rules.md`), that are used to build a system_message for the LLM.

Run iterative calls to a list of LLMs (`05-models.yaml`).

- Results are compared against baseline datasets, which are generated from the SQL queries executed against the database schema.
- The evaluation results are logged in Langfuse.
- The evaluations are reported into the \docs\results folder for later analysis.

## Dependencies

The project relies on the following Python packages:

- see `requirements.txt` for the complete list.

## Modules and Functionalities

### `llms_evaluator`
This module contains the `LLMsEvaluator` class, which is central to the evaluation process. It:
- Loads questions from YAML files.
- Parses database schemas.
- Applies semantic rules.
- Interacts with LLMs to generate SQL queries.
- Compares generated queries with baseline datasets.

### `module_utils`
Provides utility functions for:
- Consolidating files by iteration or model.
- Normalizing numeric columns.
- Aligning columns by the first row.
- Generating performance reports.

### Other Modules
- `module_data`: Handles dynamic SQL generation.
- `module_azure_openai`: Facilitates interaction with Azure OpenAI services.

## Key Classes and Methods


### `Questions` Class

The `Questions` class is responsible for loading and managing questions from a YAML file. It provides methods to:
- Load questions from a specified YAML file.
- Retrieve questions by their IDs.
- Get all questions as a list.

The YAML file should contain a list of questions; this is a sample question:

```yaml

questions:
  - question_number: 1
    user_question: |
      Which customers from the 'BUILDING' market segment placed more than 10 orders in 1996? Order by total order value descending.
    sql_query: |
      SELECT
          c.c_custkey AS customer_id,
          c.c_name    AS customer_name,
          COUNT(o.o_orderkey) AS num_orders,
          SUM(o.o_totalprice) AS total_amount
      FROM customer c
      JOIN orders o ON c.c_custkey = o.o_custkey
      WHERE c.c_mktsegment = 'BUILDING'
        AND YEAR(o.o_orderdate) = 1996
      GROUP BY c.c_custkey, c.c_name
      HAVING COUNT(o.o_orderkey) > 10
      ORDER BY total_amount DESC;
    tables_used:
      - "customer"
      - "orders"

```

Where `question_number` is the identifier for the question, `user_question` is the natural language question to test with the llm, `sql_query` is the expected SQL query [reviewed by an expert], and `tables_used` lists the tables involved in the query. `tables_used` is neccesary in order to add the related tables to the system message for the LLM.

Note: ALL fields are required.

The class provides methods to:

- Load questions from a YAML file.
- Retrieve a question by its number.
- Get all questions as a list.
- Save questions to a YAML file.

The saved file contains a structure with these properties. The functionality to save questions is useful for logging the questions used in the evaluation process.
Properties are self-descriptive: for example, `llm_sql_query` indicates the SQL query generated by the LLM; `duration_sql` is the time that took execute the query in the database, `duration_llm` indicate the time taken from the LLM to create the sentence.

```yaml

    'iteration'
    'model_name'
    'question_number'
    'user_question'
    'sql_query'
    'llm_sql_query'
    'tables_used'
    'executed'
    'llm_sql_query_changed'
    'rows'
    'columns'
    'percent_rows_equality'
    'percent_columns_equality'
    'percent_source_rows_equality'
    'percent_llm_rows_equality'
    'duration_sql'
    'duration_llm'
    'prompt_tokens'
    'completion_tokens'
    'total_tokens'
    'cost_input_EUR'
    'cost_output_EUR'
    'cost_total_EUR'

```

### `Database_schema_tables` Class
The `Database_schema_tables` class is responsible for parsing and managing the list tables from the database. It provides methods to:
- Load the schema from a YAML file.
- Retrieve tables and their columns.
- Get the schema as a dictionary for easy access.

Sample YAML structure for the database schema:

```yaml


tables:
  - name: "region"
    script: |
      create table region (
          r_regionkey integer not null,
          r_name char(25) not null,
          r_comment varchar(152),
          primary key (r_regionkey)
      );

  - name: "nation"
    script: |
      create table nation (
          n_nationkey integer not null,
          n_name char(25) not null,
          n_regionkey integer not null,
          n_comment varchar(152),
          primary key (n_nationkey)
      );

```


### `LLMsEvaluator`
The `LLMsEvaluator` class is initialized with paths to configuration files, including:
- Questions (`01-questions.yaml`)
- Database schema (`02-database_schema.yaml`)
- Semantic rules (`03-semantic-rules.md`)
- System messages (`04-system_message.md`)
- Models (`05-models.yaml`)

Key methods:
- `load_baseline_datasets`: Loads baseline datasets for comparison.
- `execute_queries`: Executes SQL queries to establish a baseline.

### `module_llm` module

The `module_llm` module provides functionality to interact with LLM services. It includes the function `get_chat_completion_from_platform`, that implements the call to the LLM using the platform API.

The funtion implements call to Anthropic, OpenAI, and Deepseek.

The function is decorate with the @observe parameter from Langfuse, which allows tracking the performance of the LLM calls. This is useful for logging and analyzing the performance of different LLMs during evaluations.


### `module_data` module

### `module_utils` module


### `main_evaluation.py`

The `main_evaluation.py` script orchestrates the evaluation process. It:
1. Initializes the `LLMsEvaluator` class with configuration files.
2. Loads baseline datasets.
3. Iterates through LLMs to generate SQL queries.
4. Compares results and generates performance reports.

## Contribution Guidelines

Developers interested in contributing can follow these steps:
1. Clone the repository.
2. Install dependencies using `pip install -r requirements.txt`.
3. Explore the `main_evaluation.py` script and `llms_evaluator` module.
4. Add new models or improve evaluation methods.
5. Submit pull requests with detailed descriptions of changes.

For any questions, refer to the documentation in the `docs/` directory.

