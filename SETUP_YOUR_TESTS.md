# üöÄ Setting Up Your Tests

Welcome! This guide will help you configure, run, and analyze tests for language models (LLMs) in this project. Let‚Äôs make your evaluation process smooth and insightful! ‚ú®

## Solution Architecture

```mermaid
graph TD
    A[User] -->|Defines Test Configurations| B[01-questions.yaml]
    A -->|Updates Database Schema| C[02-database_schema.yaml]
    A -->|Defines Semantic Rules| D[03-semantic-rules.yaml]
    A -->|Modifies System Messages| E[04-system_message.yaml]
    A -->|Adds/Updates Models| F[05-models.yaml]
    
    B -->|Used in Tests| G[Evaluation]
    C -->|Used in Tests| G
    D -->|Used in Tests| G
    E -->|Used in Tests| G
    F -->|Used in Tests| G
    
    G -->|Generates| H[performance_report.txt]
    G -->|Generates| I[results_llm_iter_model_name.yaml]
    
    H -->|Analyzed by| A
    I -->|Analyzed by| A
```

This diagram illustrates how the user interacts with the YAML files to define test configurations, which are then processed by evaluation scripts to generate performance reports for analysis.

---

## üìù Testing Procedure Overview

| Step | Description |
|------|-------------|
| 1Ô∏è‚É£  | **Define Test Configuration**<br>Choose models, queries, and datasets to evaluate. |
| 2Ô∏è‚É£  | **Execute Tests**<br>Run scripts to collect performance metrics. |
| 3Ô∏è‚É£  | **Analyze Results**<br>Review reports to understand and improve model performance. |

---

## üìÇ YAML Files in the `results` Directory

| File | Purpose |
|------|---------|
| `01-questions.yaml` | üóíÔ∏è List of questions/queries with IDs, text, and expected outputs. |
| `02-database_schema.yaml` | üèóÔ∏è Database schema: tables, columns, and data types. |
| `03-semantic-rules.yaml` | üß† Semantic rules for evaluating response correctness. |
| `04-system_message.yaml` | üí¨ System-level messages/prompts for tests. |
| `05-models.yaml` | ü§ñ Models to test, with configs (enabled, cost, etc.). |

---

## üìä Results & Analysis

After running tests, results appear in the `results` directory. These include:

#### Generated YAML Files
| File | Description |
|------|-------------|
| `results_llm_[iter]_{model_name}.yaml` | Contains detailed results for each model tested. Each file includes metrics such as execution time, token usage, and accuracy for the queries executed. |

#### Columns in the Results
The generated YAML files include the following columns:

Columns in the Results
| Column Name           | Description                                                                                  |
|-----------------------|----------------------------------------------------------------------------------------------|
| `iteration`           | Iteration number of the test.                                                                |
| `model_name`          | Name of the model being tested.                                                              |
| `question_number`     | Unique identifier for the question executed.                                                 |
| `user_question`       | The original question provided by the user.                                                  |
| `sql_query`           | The SQL query generated by the user or system.                                               |
| `llm_sql_query`       | The SQL query generated by the language model.                                               |
| `tables_used`         | List of database tables used in the query.                                                   |
| `executed`            | Boolean indicating whether the query was executed successfully.                              |
| `llm_sql_query_changed` | Boolean indicating if the LLM's SQL query was modified before execution.                   |
| `rows`                | Number of rows returned by the query.                                                        |
| `columns`             | Number of columns returned by the query.                                                     |
| `rows_equality`       | Equality score for rows between expected and actual results.                                 |
| `columns_equality`    | Equality score for columns between expected and actual results.                              |
| `datasets_equality`   | Overall equality score for the dataset between expected and actual results.                  |
| `duration_sql`        | Time taken to execute the SQL query.                                                         |
| `duration_llm`        | Time taken by the LLM to generate the SQL query.                                             |
| `prompt_tokens`       | Number of tokens used in the prompt sent to the LLM.                                         |
| `completion_tokens`   | Number of tokens in the LLM's response.                                                      |
| `total_tokens`        | Total number of tokens used (prompt + completion).                                           |
| `cost_input_EUR`      | Cost of input tokens in Euros.                                                               |
| `cost_output_EUR`     | Cost of output tokens in Euros.                                                              |
| `cost_total_EUR`      | Total cost of tokens in Euros.                                                               |

These files provide granular insights into the performance of each model, enabling detailed analysis and comparison.

### üèÖ Performance Report (`performance_report.txt`)

| Section | Metrics |
|---------|---------|
| **Per Model** | <ul><li>üî¢ `queries_executed`</li><li>‚è±Ô∏è `mean_sql_time`</li><li>ü§ñ `mean_llm_time`</li><li>üî§ `mean_tokens`</li><li>üü∞ `mean_datasets_equality`</li><li>üí∂ `mean_cost_EUR`</li></ul> |
| **Per Query** | <ul><li>ü§ñ `mean_llm_time`</li><li>üü∞ `mean_datasets_equality`</li><li>üìä `mean_rows_equality`</li><li>üìä `mean_columns_equality`</li></ul> |

- üåü **Best Models**: Quickly spot top performers by average LLM time and other metrics!

### üìà Understanding the Sample Performance Report

The `performance_report.txt` file provides detailed metrics for each model and query. Here‚Äôs how to interpret the data:

#### Per Model Analysis
| Metric | Explanation |
|--------|-------------|
| **`queries_executed`** | Number of queries executed for the model. Higher values indicate more comprehensive testing. |
| **`mean_sql_time`** | Average time taken for SQL execution. Lower values are better for database efficiency. |
| **`mean_llm_time`** | Average time taken by the model to generate responses. Faster models are preferred for real-time applications. |
| **`mean_tokens`** | Average number of tokens used per query. Models with fewer tokens may be more cost-efficient. |
| **`mean_datasets_equality`** | Average similarity between the model‚Äôs output and the expected dataset. Higher values indicate better accuracy. |
| **`mean_cost_EUR`** | Average cost per query in Euros. Lower costs are ideal for budget-conscious projects. |

**Example:**
- `DeepSeek-V3-0324` has a low cost (`‚Ç¨0.002916`) and high accuracy (`0.77`), making it a cost-effective choice.
- `Llama-3.3-70B-Instruct` has high accuracy (`0.67`) but is expensive (`‚Ç¨0.011145`) and slow (`28.38s mean LLM time`).

#### Per Query Analysis
| Metric | Explanation |
|--------|-------------|
| **`mean_llm_time`** | Average time taken by the model for each query. Lower values are better for responsiveness. |
| **`mean_datasets_equality`** | Similarity between the model‚Äôs output and the expected dataset. Higher values indicate better accuracy. |
| **`mean_rows_equality`** and **`mean_columns_equality`** | Metrics for row and column-level accuracy. Useful for understanding granular correctness. |

**Example:**
- Query `1` has high accuracy (`0.94`) and low response time (`3.38s`), making it ideal for real-time use.
- Query `5` has low accuracy (`0`) and high response time (`6.06s`), indicating potential issues with the model or dataset.

#### Best Models
The report highlights the best-performing models based on average LLM time and other metrics. For example:
- `gpt-4.1` has a low mean LLM time (`1.84s`) and high accuracy (`0.85`), making it a strong candidate for real-time applications.
- `grok-3-mini` has a high mean LLM time (`16.49s`) but low cost (`‚Ç¨0.000662`), which might be suitable for batch processing.

### Decision-Making Tips
1. **Real-Time Applications**: Prioritize models with low `mean_llm_time` and high `mean_datasets_equality`.
2. **Cost Efficiency**: Look for models with low `mean_cost_EUR` and acceptable accuracy.
3. **Batch Processing**: Consider models with higher response times but lower costs for non-time-sensitive tasks.

By analyzing these metrics, you can select the models that best fit your project‚Äôs requirements and constraints.

---

## üõ†Ô∏è Steps to Create Your Own Tests

| Step | Action |
|------|--------|
| 1Ô∏è‚É£ | **Edit YAML Files**<br>- Add questions to `01-questions.yaml`<br>- Update schema in `02-database_schema.yaml`<br>- Define rules in `03-semantic-rules.yaml`<br>- Modify messages in `04-system_message.yaml`<br>- Add/update models in `05-models.yaml` |
| 2Ô∏è‚É£ | **Run Tests**<br>Use provided scripts to execute and generate results. |
| 3Ô∏è‚É£ | **Analyze Results**<br>Check `performance_report.txt` for insights.<br>Compare metrics to find strengths and weaknesses. |

---

By following these steps, you‚Äôll tailor the testing process to your needs and unlock valuable insights into your models‚Äô performance! üéØ

