Performance Report per model:

| model                                  |   queries_executed |   mean_sql_time |   mean_llm_time |   stdev_llm_time |   mean_tokens |   mean_source_rows_equality |   mean_llm_rows_equality |   mean_cost_EUR |
|:---------------------------------------|-------------------:|----------------:|----------------:|-----------------:|--------------:|----------------------------:|-------------------------:|----------------:|
| DeepSeek-V3-0324                       |                 20 |            2.8  |           13.82 |            17.72 |       2045.7  |                        0.67 |                     0.7  |        0.002937 |
| Llama-3.3-70B-Instruct                 |                 20 |            2.52 |            3.72 |             1.69 |       1976.7  |                        0.65 |                     0.65 |        0.01115  |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |                 20 |            2.94 |            4.65 |             1.94 |       1958.8  |                        0.65 |                     0.65 |        0.000316 |
| Llama-4-Scout-17B-16E-Instruct         |                 20 |            2.42 |            8.56 |             6.94 |       2020.15 |                        0.6  |                     0.55 |        0.000536 |
| Mistral-Large-2411                     |                 20 |            2.31 |            5.76 |             3.73 |       2580.45 |                        0.62 |                     0.65 |        0.001832 |
| claude-3-5-haiku-20241022              |                 20 |            2.55 |            3.74 |             1.55 |       2472.15 |                        0.65 |                     0.65 |        0.002775 |
| claude-3-5-sonnet-20241022             |                 20 |            2.75 |            3.93 |             1.5  |       2446.6  |                        0.8  |                     0.8  |        0.010024 |
| claude-3-7-sonnet-20250219             |                 20 |            2.9  |            4.74 |             3.07 |       2446.25 |                        0.77 |                     0.77 |        0.010018 |
| claude-sonnet-4-20250514               |                 20 |            2.61 |            4.12 |             1.4  |       2450.15 |                        0.75 |                     0.75 |        0.010077 |
| gpt-4.1                                |                 20 |            2.81 |            6.73 |            10.79 |       1972.3  |                        0.75 |                     0.75 |        0.005025 |
| gpt-4.1-mini                           |                 20 |            2.74 |            2.82 |             1.73 |       1980    |                        0.75 |                     0.75 |        0.001017 |
| gpt-4o                                 |                 20 |            2.5  |           11    |            15.74 |       1961.95 |                        0.8  |                     0.8  |        0.006178 |
| gpt-4o-mini                            |                 20 |            2.58 |            4.69 |             7.47 |       1965.65 |                        0.65 |                     0.65 |        0.000373 |
| grok-3                                 |                 20 |            3.01 |            2.96 |             0.93 |       1941.05 |                        0.85 |                     0.85 |        0.007909 |
| grok-3-mini                            |                 20 |            3.38 |           15.88 |            10.32 |       4518.95 |                        0.8  |                     0.8  |        0.000665 |

Performance Report per query:

|   question |   mean_llm_time |   stdev_llm_time |   mean_source_rows_equality |   mean_llm_rows_equality |   mean_rows_equality |   mean_columns_equality |
|-----------:|----------------:|-----------------:|----------------------------:|-------------------------:|---------------------:|------------------------:|
|          1 |            3.09 |             1.23 |                        1    |                     1    |                 1    |                    1    |
|          2 |            3.32 |             1.69 |                        1    |                     1    |                 1    |                    1    |
|          3 |            2.58 |             1.87 |                        1    |                     1    |                 1    |                    1    |
|          4 |            6.84 |             5.48 |                        0.6  |                     0.62 |                 0.58 |                    0.8  |
|          5 |            8.41 |            11.49 |                        0    |                     0    |                 0.47 |                    0.43 |
|          6 |            4.84 |             4.23 |                        0.4  |                     0.4  |                 0.93 |                    0.76 |
|          7 |            4.88 |             7.35 |                        0.87 |                     0.87 |                 1    |                    0.96 |
|          8 |            4.03 |             2.24 |                        1    |                     1    |                 1    |                    1    |
|          9 |            8.37 |            11.99 |                        0.87 |                     0.87 |                 0.87 |                    0.87 |
|         10 |            4.43 |             3.23 |                        1    |                     1    |                 1    |                    1    |
|         11 |            2.79 |             2.4  |                        1    |                     1    |                 1    |                    1    |
|         12 |            2.83 |             2.25 |                        1    |                     1    |                 1    |                    1    |
|         13 |           10.17 |            11.48 |                        0.93 |                     0.93 |                 0.93 |                    0.93 |
|         14 |            6.06 |             5.47 |                        0.87 |                     0.87 |                 0.87 |                    1    |
|         15 |           11.43 |            19.92 |                        0.93 |                     0.93 |                 0.93 |                    0.93 |
|         16 |            9.01 |             6.82 |                        0.47 |                     0.47 |                 0.67 |                    0.7  |
|         17 |           13.37 |            11.8  |                        0    |                     0    |                 0.83 |                    0.72 |
|         18 |            6.47 |             6.51 |                        0.53 |                     0.53 |                 0.93 |                    0.83 |
|         19 |            9.26 |             8.31 |                        0.27 |                     0.27 |                 0.47 |                    0.41 |
|         20 |            7.31 |            11.03 |                        0.6  |                     0.6  |                 0.87 |                    0.85 |

Best models based on average LLM time:


| model                                  |   mean_llm_time |
|:---------------------------------------|----------------:|
| gpt-4.1-mini                           |            2.82 |
| grok-3                                 |            2.96 |
| Llama-3.3-70B-Instruct                 |            3.72 |
| claude-3-5-haiku-20241022              |            3.74 |
| claude-3-5-sonnet-20241022             |            3.93 |
| claude-sonnet-4-20250514               |            4.12 |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |            4.65 |
| gpt-4o-mini                            |            4.69 |
| claude-3-7-sonnet-20250219             |            4.74 |
| Mistral-Large-2411                     |            5.76 |
| gpt-4.1                                |            6.73 |
| Llama-4-Scout-17B-16E-Instruct         |            8.56 |
| gpt-4o                                 |           11    |
| DeepSeek-V3-0324                       |           13.82 |
| grok-3-mini                            |           15.88 |

Best models based on mean token cost:

| model                                  |   mean_cost_EUR |
|:---------------------------------------|----------------:|
| Llama-4-Maverick-17B-128E-Instruct-FP8 |        0.000316 |
| gpt-4o-mini                            |        0.000373 |
| Llama-4-Scout-17B-16E-Instruct         |        0.000536 |
| grok-3-mini                            |        0.000665 |
| gpt-4.1-mini                           |        0.001017 |
| Mistral-Large-2411                     |        0.001832 |
| claude-3-5-haiku-20241022              |        0.002775 |
| DeepSeek-V3-0324                       |        0.002937 |
| gpt-4.1                                |        0.005025 |
| gpt-4o                                 |        0.006178 |
| grok-3                                 |        0.007909 |
| claude-3-7-sonnet-20250219             |        0.010018 |
| claude-3-5-sonnet-20241022             |        0.010024 |
| claude-sonnet-4-20250514               |        0.010077 |
| Llama-3.3-70B-Instruct                 |        0.01115  |

Best models based on average data rows equality:


| model                                  |   mean_source_rows_equality |
|:---------------------------------------|----------------------------:|
| grok-3                                 |                        0.85 |
| gpt-4o                                 |                        0.8  |
| claude-3-5-sonnet-20241022             |                        0.8  |
| grok-3-mini                            |                        0.8  |
| claude-3-7-sonnet-20250219             |                        0.77 |
| gpt-4.1                                |                        0.75 |
| gpt-4.1-mini                           |                        0.75 |
| claude-sonnet-4-20250514               |                        0.75 |
| DeepSeek-V3-0324                       |                        0.67 |
| Llama-3.3-70B-Instruct                 |                        0.65 |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |                        0.65 |
| claude-3-5-haiku-20241022              |                        0.65 |
| gpt-4o-mini                            |                        0.65 |
| Mistral-Large-2411                     |                        0.62 |
| Llama-4-Scout-17B-16E-Instruct         |                        0.6  |


Ranking of the models based on the total cost, LLM time and source rows equality:

| model                                  | rank_quality   | rank_time   | rank_price   |
|:---------------------------------------|:---------------|:------------|:-------------|
| grok-3                                 | 1 (0.85)       | 2 (2.96)    |              |
| claude-3-5-sonnet-20241022             | 2 (0.8)        | 5 (3.93)    |              |
| grok-3-mini                            | 2 (0.8)        |             | 4 (0.000665) |
| gpt-4o                                 | 2 (0.8)        |             |              |
| claude-3-7-sonnet-20250219             | 5 (0.77)       |             |              |
| gpt-4.1-mini                           | 6 (0.75)       | 1 (2.82)    | 5 (0.001017) |
| claude-sonnet-4-20250514               | 6 (0.75)       | 6 (4.12)    |              |
| gpt-4.1                                | 6 (0.75)       |             |              |
| Llama-3.3-70B-Instruct                 |                | 3 (3.72)    |              |
| claude-3-5-haiku-20241022              |                | 4 (3.74)    | 7 (0.002775) |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |                | 7 (4.65)    | 1 (0.000316) |
| gpt-4o-mini                            |                | 8 (4.69)    | 2 (0.000373) |
| Llama-4-Scout-17B-16E-Instruct         |                |             | 3 (0.000536) |
| Mistral-Large-2411                     |                |             | 6 (0.001832) |
| DeepSeek-V3-0324                       |                |             | 8 (0.002937) |