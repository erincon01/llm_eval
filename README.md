# llm_eval

## Project Overview

This project is designed to evaluate the performance of various Large Language Models (LLMs) in generating SQL queries based on predefined questions and database schemas. It provides tools for orchestrating evaluations, comparing results, and generating performance reports.

## What is implemented

- evaluation of expert SQL queries against LLM-generated SQL queries.
- comparison of LLM-generated SQL queries against baseline datasets.
- logging of evaluation results in Langfuse.
- reporting of evaluations into the `docs/results` folder for later analysis.

## Roadmap

- general classes review and refactor.
- general devops practices:
  - add pre-commit hooks.
  - add type checking with mypy.
  - add unit tests.
  - add integration tests.
  - add code coverage report.
- proper keys handing in the models.yaml configuration file.
- add support to Gemini models.
- add support to on-prem models (Ollama).
- bettter datasets comparison (function `compare_baseline_resultset_with_LLM_resultset` in `llm_evaluator` class). Current limitations:
  - comparison is sensible to ordered rows.
  - comparison is sensible to columns in different orders.
  - comparison is sensible when the data row value is not exactly the same (for example: 9.0122 vs 9.0123 are different but 0 equality weight is given).
- facade app based on streamlit or similar with useful charts.
- support for more SQL Engines (PostgreSQL, Microsoft Fabric, Databricks, Snowflake, MySQL, etc.).

## The process

- Given a set of questions (`01-questions.yaml`),
- Given a database schema (`02-database_schema.yaml`),
- Given a list of semantic rules (`03-semantic-rules.md`), that are used to build a system_message for the LLM.

Run iterative calls to a list of LLMs (`05-models.yaml`).

- Results are compared against baseline datasets, which are generated from the SQL queries executed against the database schema.
- The evaluation results are logged in Langfuse.
- The evaluations are reported into the \docs\results folder for later analysis.

## Dependencies

The project relies on the following Python packages:

- see `requirements.txt`.

## Modules, classes and Functionalities

### `Questions` Class

The `Questions` class is responsible for loading and managing questions from a YAML file. It provides methods to:

- Load questions from a specified YAML file.
- Retrieve questions by their IDs.
- Get all questions as a list.

The YAML file should contain a list of questions; this is a sample question:

```yaml

questions:
  - question_number: 1
    user_question: |
      Which customers from the 'BUILDING' market segment placed more than 10 orders in 1996? Order by total order value descending.
    sql_query: |
      SELECT
          c.c_custkey AS customer_id,
          c.c_name    AS customer_name,
          COUNT(o.o_orderkey) AS num_orders,
          SUM(o.o_totalprice) AS total_amount
      FROM customer c
      JOIN orders o ON c.c_custkey = o.o_custkey
      WHERE c.c_mktsegment = 'BUILDING'
        AND YEAR(o.o_orderdate) = 1996
      GROUP BY c.c_custkey, c.c_name
      HAVING COUNT(o.o_orderkey) > 10
      ORDER BY total_amount DESC;
    tables_used:
      - "customer"
      - "orders"

```

Where `question_number` is the identifier for the question, `user_question` is the natural language question to test with the llm, `sql_query` is the expected SQL query [reviewed by an expert], and `tables_used` lists the tables involved in the query. `tables_used` is neccesary in order to add the related tables to the system message for the LLM.

Note: ALL fields are required.

The class provides methods to:

- Load questions from a YAML file.
- Retrieve a question by its number.
- Get all questions as a list.
- Save questions to a YAML file.

The saved file contains a structure with these properties. The functionality to save questions is useful for logging the questions used in the evaluation process.
Properties are self-descriptive: for example, `llm_sql_query` indicates the SQL query generated by the LLM; `duration_sql` is the time that took execute the query in the database, `duration_llm` indicate the time taken from the LLM to create the sentence.

```yaml

    'iteration'
    'model_name'
    'question_number'
    'user_question'
    'sql_query'
    'llm_sql_query'
    'tables_used'
    'executed'
    'llm_sql_query_changed'
    'rows'
    'columns'
    'percent_rows_equality'
    'percent_columns_equality'
    'percent_source_rows_equality'
    'percent_llm_rows_equality'
    'duration_sql'
    'duration_llm'
    'prompt_tokens'
    'completion_tokens'
    'total_tokens'
    'cost_input_EUR'
    'cost_output_EUR'
    'cost_total_EUR'

```

### `Database_schema_tables` Class

The `Database_schema_tables` class is responsible for parsing and managing the list tables from the database. It provides methods to:

- Load the schema from a YAML file.
- Retrieve tables and their columns.
- Get the schema as a dictionary for easy access.

Sample YAML structure for the database schema:

```yaml

tables:
  - name: "region"
    script: |
      create table region (
          r_regionkey integer not null,
          r_name char(25) not null,
          r_comment varchar(152),
          primary key (r_regionkey)
      );

  - name: "nation"
    script: |
      create table nation (
          n_nationkey integer not null,
          n_name char(25) not null,
          n_regionkey integer not null,
          n_comment varchar(152),
          primary key (n_nationkey)
      );

```

### `LLMsEvaluator`

This module contains the `LLMsEvaluator` class, which is central to the evaluation process. It:

The `LLMsEvaluator` class is initialized with paths to configuration files, including:

- Loading questions (`01-questions.yaml`). See the `Questions` class for details.
- Loading database schema (`02-database_schema.yaml`). See the `Database_schema_tables` class for details.
- Loading semantic rules (`03-semantic-rules.md`). Semantic rules are a set of guidelines that define how to map natural language terms to SQL constructs, ensuring consistency and accuracy in query generation.
- Loading system messages (`04-system_message.md`). This is the system message that will be used to prompt the LLM. It has instructions for the LLM on how to build the SQL query.
- Loading models (`05-models.yaml`). This file contains the configuration for the different LLM models that can be used in the evaluation process. The models follow this structure:

```yaml
models_configs:
  - id: <configuration_group>
    enabled: true|false
    endpoint: <endpoint_url>
    api_key: <api_key>
    models:
      - name: <model_name>
        enabled: true|false
        cost_input_tokens_EUR_1K:  <decimal_value>
        cost_output_tokens_EUR_1K: <decimal_value>
```

- enabled: optional value, indicates if the model or configuration group is enabled for evaluation.
- other properties are self-descriptive.
- currently, the project supports EUR only.

#### Methods Overview

- `__init__`: Initializes the class with configuration file paths and loads necessary data.
- `__load_file`: Reads the content of a file as a string.
- `__load_questions`: Loads questions from a YAML file.
- `__load_models_from_yaml`: Loads and validates model configurations from a YAML file.
- `__get_sql_query_from_LLM`: Generates SQL queries using an LLM based on a user prompt and context.
- `__remove_baseline_datasets`: Removes baseline datasets and summary files from a specified directory.
- `compare_baseline_resultset_with_LLM_resultset`: Compares LLM-generated results with baseline datasets.
- `process_questions_with_model`: Processes questions using a specific model and logs the results.
- `evaluate_models`: Iterates through all models and evaluates their performance on the loaded questions.
- `execute_queries`: Executes SQL queries from the questions file and exports results to CSV files.
- `load_baseline_datasets`: Loads baseline datasets from a specified directory for comparison.

### `module_llm` module

The `module_llm` module provides functionality to interact with LLM services. It includes the function `get_chat_completion_from_platform`, that implements the call to the LLM using the platform API.

The funtion implements call to Anthropic, OpenAI, and Deepseek.

The function is decorate with the @observe parameter from Langfuse, which allows tracking the performance of the LLM calls. This is useful for logging and analyzing the performance of different LLMs during evaluations.

### `module_utils`

Provides utility functions for:

- Consolidating files by iteration or model.
- Normalizing numeric columns.
- Aligning columns by the first row.
- Generating performance reports.

### `module_data` module

The `module_data` module contains useful functions to interact with SQL Server.
it is reused from other projects; in this project the funtion used is `get_dynamic_sql` that executes the SQL query against the database and returns the result set.

## Contribution Guidelines

Developers interested in contributing can follow these steps:

1. Clone the repository.
2. Install dependencies using `pip install -r requirements.txt`.
3. Explore the `main_evaluation.py` script and `llms_evaluator` module.
4. Add new models or improve evaluation methods.
5. Submit pull requests with detailed descriptions of changes.

For any questions, refer to the documentation in the `docs/` directory.
