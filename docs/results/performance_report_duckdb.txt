Data source: duckdb

Performance Report per model:

| model                                  |   queries_executed |   mean_sql_time |   mean_llm_time |   stdev_llm_time |   mean_tokens |   mean_datasets_equality |   mean_cost_EUR |
|:---------------------------------------|-------------------:|----------------:|----------------:|-----------------:|--------------:|-------------------------:|----------------:|
| DeepSeek-V3-0324                       |                 20 |            3.81 |            5.66 |             9.4  |       2222.2  |                     0.7  |        0.003129 |
| Llama-3.3-70B-Instruct                 |                 20 |            3.06 |            4.25 |             2.62 |       2156    |                     0.55 |        0.01222  |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |                 20 |            2.66 |            4.09 |             8.35 |       2139.55 |                     0.65 |        0.000342 |
| Llama-4-Scout-17B-16E-Instruct         |                 20 |            2.88 |           58.88 |           252.35 |       2161.7  |                     0.57 |        0.000545 |
| Mistral-Large-2411                     |                 20 |            3.04 |            4.85 |             2.38 |       2822.85 |                     0.7  |        0.002004 |
| Phi-3-medium-128k-instruct             |                 20 |            0    |           93.86 |           389.87 |        423.85 |                     0    |        0.000227 |
| claude-3-5-haiku-20241022              |                 20 |            1.87 |            4.96 |             2.11 |       2666.9  |                     0.72 |        0.002885 |
| claude-3-5-sonnet-20241022             |                 20 |            3.49 |            4.05 |             1.36 |       2652.05 |                     0.85 |        0.010597 |
| claude-3-7-sonnet-20250219             |                 20 |            4.78 |            4.26 |             1.43 |       2652.7  |                     0.65 |        0.010607 |
| claude-sonnet-4-20250514               |                 20 |            2.5  |            4.69 |             1.56 |       2654.6  |                     0.85 |        0.010636 |
| gpt-4.1                                |                 20 |            2.58 |            1.73 |             0.64 |       2147.3  |                     0.7  |        0.005351 |
| gpt-4.1-mini                           |                 20 |            2.09 |            1.76 |             0.56 |       2143.25 |                     0.7  |        0.001064 |
| gpt-4o                                 |                 20 |            1.9  |            2.52 |             1.03 |       2145.85 |                     0.7  |        0.006674 |
| gpt-4o-mini                            |                 20 |            1.54 |            1.99 |             0.54 |       2131.9  |                     0.55 |        0.000392 |
| gpt-oss-120b                           |                 20 |            1.54 |            6.38 |             3.71 |       2776.55 |                     0.75 |        0.002293 |
| grok-3                                 |                 20 |           50.3  |            3.68 |             1.73 |       2121.3  |                     0.75 |        0.008465 |
| grok-3-mini                            |                 20 |            2.28 |           18.72 |            11.29 |       5009.45 |                     0.75 |        0.000711 |

Performance Report per query:

|   question |   mean_llm_time |   stdev_llm_time |   mean_rows_equality |   mean_columns_equality |   mean_datasets_equality |
|-----------:|----------------:|-----------------:|---------------------:|------------------------:|-------------------------:|
|          1 |            3.38 |             1.71 |                 0.91 |                    0.94 |                     0.91 |
|          2 |            3.74 |             4.7  |                 0.94 |                    0.94 |                     0.94 |
|          3 |            2.53 |             1.68 |                 0.94 |                    0.94 |                     0.94 |
|          4 |           72.54 |           272.89 |                 0.71 |                    0.88 |                     0.73 |
|          5 |            6.26 |             6.88 |                 0.65 |                    0.5  |                     0.06 |
|          6 |            3.54 |             1.87 |                 0.94 |                    0.88 |                     0.76 |
|          7 |            2.68 |             1.93 |                 0.88 |                    0.69 |                     0.29 |
|          8 |            3.12 |             1.93 |                 0.94 |                    0.94 |                     0.94 |
|          9 |            4.56 |             3.73 |                 0.83 |                    0.84 |                     0.82 |
|         10 |            3.46 |             2.52 |                 0.88 |                    0.91 |                     0.76 |
|         11 |            2.69 |             2.46 |                 0.94 |                    0.71 |                     0.71 |
|         12 |            2.79 |             2.1  |                 0.94 |                    0.94 |                     0.94 |
|         13 |            4.48 |             2.17 |                 0.94 |                    0.94 |                     0.94 |
|         14 |            4.58 |             4.95 |                 0.82 |                    0.92 |                     0.77 |
|         15 |            8.96 |            10.05 |                 0.94 |                    0.94 |                     0.94 |
|         16 |            8.21 |             8.34 |                 0.47 |                    0.44 |                     0.35 |
|         17 |            6.26 |             5.19 |                 0.9  |                    0.71 |                     0    |
|         18 |          108.08 |           423.2  |                 0.94 |                    0.83 |                     0.53 |
|         19 |            8.28 |             8.63 |                 0.64 |                    0.68 |                     0.41 |
|         20 |            6.12 |             9.68 |                 0.53 |                    0.46 |                     0.35 |

Best models based on average LLM time:

| model                                  |   mean_llm_time |
|:---------------------------------------|----------------:|
| gpt-4.1                                |            1.73 |
| gpt-4.1-mini                           |            1.76 |
| gpt-4o-mini                            |            1.99 |
| gpt-4o                                 |            2.52 |
| grok-3                                 |            3.68 |
| claude-3-5-sonnet-20241022             |            4.05 |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |            4.09 |
| Llama-3.3-70B-Instruct                 |            4.25 |
| claude-3-7-sonnet-20250219             |            4.26 |
| claude-sonnet-4-20250514               |            4.69 |
| Mistral-Large-2411                     |            4.85 |
| claude-3-5-haiku-20241022              |            4.96 |
| DeepSeek-V3-0324                       |            5.66 |
| gpt-oss-120b                           |            6.38 |
| grok-3-mini                            |           18.72 |
| Llama-4-Scout-17B-16E-Instruct         |           58.88 |
| Phi-3-medium-128k-instruct             |           93.86 |

Best models based on mean token cost:

| model                                  |   mean_cost_EUR |
|:---------------------------------------|----------------:|
| Phi-3-medium-128k-instruct             |        0.000227 |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |        0.000342 |
| gpt-4o-mini                            |        0.000392 |
| Llama-4-Scout-17B-16E-Instruct         |        0.000545 |
| grok-3-mini                            |        0.000711 |
| gpt-4.1-mini                           |        0.001064 |
| Mistral-Large-2411                     |        0.002004 |
| gpt-oss-120b                           |        0.002293 |
| claude-3-5-haiku-20241022              |        0.002885 |
| DeepSeek-V3-0324                       |        0.003129 |
| gpt-4.1                                |        0.005351 |
| gpt-4o                                 |        0.006674 |
| grok-3                                 |        0.008465 |
| claude-3-5-sonnet-20241022             |        0.010597 |
| claude-3-7-sonnet-20250219             |        0.010607 |
| claude-sonnet-4-20250514               |        0.010636 |
| Llama-3.3-70B-Instruct                 |        0.01222  |

Best models based on average datasets equality:

| model                                  |   mean_datasets_equality |
|:---------------------------------------|-------------------------:|
| claude-3-5-sonnet-20241022             |                     0.85 |
| claude-sonnet-4-20250514               |                     0.85 |
| gpt-oss-120b                           |                     0.75 |
| grok-3-mini                            |                     0.75 |
| grok-3                                 |                     0.75 |
| claude-3-5-haiku-20241022              |                     0.72 |
| DeepSeek-V3-0324                       |                     0.7  |
| Mistral-Large-2411                     |                     0.7  |
| gpt-4o                                 |                     0.7  |
| gpt-4.1                                |                     0.7  |
| gpt-4.1-mini                           |                     0.7  |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |                     0.65 |
| claude-3-7-sonnet-20250219             |                     0.65 |
| Llama-4-Scout-17B-16E-Instruct         |                     0.57 |
| Llama-3.3-70B-Instruct                 |                     0.55 |
| gpt-4o-mini                            |                     0.55 |
| Phi-3-medium-128k-instruct             |                     0    |


Ranking of the models based on the total cost, LLM time and source rows equality:

| model                                  | rank_quality   | rank_time   | rank_price   |
|:---------------------------------------|:---------------|:------------|:-------------|
| claude-3-5-sonnet-20241022             | 1 (0.85)       | 6 (4.05)    |              |
| claude-sonnet-4-20250514               | 1 (0.85)       |             |              |
| grok-3                                 | 3 (0.75)       | 5 (3.68)    |              |
| grok-3-mini                            | 3 (0.75)       |             | 5 (0.000711) |
| gpt-oss-120b                           | 3 (0.75)       |             | 8 (0.002293) |
| claude-3-5-haiku-20241022              | 6 (0.72)       |             |              |
| gpt-4.1                                | 7 (0.7)        | 1 (1.73)    |              |
| gpt-4.1-mini                           | 7 (0.7)        | 2 (1.76)    | 6 (0.001064) |
| gpt-4o                                 | 7 (0.7)        | 4 (2.52)    |              |
| Mistral-Large-2411                     | 7 (0.7)        |             | 7 (0.002004) |
| DeepSeek-V3-0324                       | 7 (0.7)        |             |              |
| gpt-4o-mini                            |                | 3 (1.99)    | 3 (0.000392) |
| Llama-4-Maverick-17B-128E-Instruct-FP8 |                | 7 (4.09)    | 2 (0.000342) |
| Llama-3.3-70B-Instruct                 |                | 8 (4.25)    |              |
| Phi-3-medium-128k-instruct             |                |             | 1 (0.000227) |
| Llama-4-Scout-17B-16E-Instruct         |                |             | 4 (0.000545) |
| claude-3-7-sonnet-20250219             |                |             |              |



