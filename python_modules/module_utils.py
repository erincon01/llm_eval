import os
import re
import csv
import numpy as np
import pandas as pd

from questions import Questions
import tabulate


def remove_quotations(sql_query: str) -> str:
    """
    Remove quotations from the SQL query.

    :param sql_query: SQL query to process.
    :return: Processed SQL query, changed.
    """

    changed = False
    
    # Regex pattern to extract content between ```sql, ```code, or similar delimiters
    patterns = [
        r"```(?:sql|code)?\s*(.*?)\s*```",     # ```sql ... ```
        r"``(?:sql|code)?\s*(.*?)\s*``",       # ``sql ... ``
        r"`(?:sql|code)?\s*(.*?)\s*``",       # ``sql ... ``
    ]

    for pattern in patterns:
        match = re.search(pattern, sql_query, re.DOTALL | re.IGNORECASE)
        if match:
            extracted = match.group(1).strip()
            if extracted:
                sql_query = extracted
                changed = True
                break 

    return sql_query, changed


def normalize_numeric_columns(df1, df2):
    """
    Normalizes numeric columns in both DataFrames to the smallest shared decimal precision.

    :param baseline_df: Baseline DataFrame.
    :param llm_df: DataFrame generated by the LLM.
    :return: Normalized baseline_df and llm_df.
    """
    # Identify numeric columns in both DataFrames
    numeric_columns = df1.select_dtypes(include=[np.number]).columns.intersection(
        df2.select_dtypes(include=[np.number]).columns
    )

    for col in numeric_columns:
        # Determine the smallest decimal precision for the column in both DataFrames
        baseline_precisions = df1[col].dropna().apply(lambda x: len(str(x).split(".")[1]) if "." in str(x) else 0)
        llm_precisions = df2[col].dropna().apply(lambda x: len(str(x).split(".")[1]) if "." in str(x) else 0)

        min_precision = min(baseline_precisions.min(), llm_precisions.min())

        # Round both DataFrames to the smallest shared precision
        df1[col] = df1[col].round(min_precision)
        df2[col] = df2[col].round(min_precision)

    return df1, df2


def align_columns_by_first_row(df1, df2):
    """
    Aligns the columns of llm_df to match the order of baseline_df based on the first row's values.

    :param baseline_df: Baseline DataFrame.
    :param llm_df: DataFrame generated by the LLM.
    :return: llm_df with columns reordered to match baseline_df.
    """
    if df1.empty or df2.empty:
        return df1, df2

    # Get the first row of both DataFrames
    df1_first_row = df1.iloc[0].tolist()
    df2_first_row = df2.iloc[0].tolist()

    # Create a mapping of baseline column names to their values in the first row
    df1_mapping = {col: val for col, val in zip(df1.columns, df1_first_row)}

    # Create a mapping of llm column names to their values in the first row
    df2_mapping = {col: val for col, val in zip(df2.columns, df2_first_row)}

    # Determine the order of llm columns based on matching values in baseline
    ordered_columns = []
    for df1_col, df1_val in df1_mapping.items():
        for df2_col, df2_val in df2_mapping.items():
            if df1_val == df2_val:
                ordered_columns.append(df2_col)
                break

    # Reorder df2 columns to match the determined order
    df2 = df2[ordered_columns]

    return df1, df2


def consolidate_files_by_iteration(results_path: str, file_name_prefix: str):
    """
    Consolidate the files generated by the LLM into a single file.
    :param results_path: Path to the results folder.
    :param file_name_prefix: Prefix of the files to consolidate.
    :return: None
    """
    # Search for the generated files

    filtered_files = []
    model_names = []

    for file in os.listdir(results_path):
        file_path = os.path.join(results_path, file)
        if os.path.isfile(file_path) and file.startswith(file_name_prefix) and file.endswith(".yaml"):
            filtered_files.append(file_path)
            # Extract the model as the last part of the file name, after the last underscore and before the ".yaml" extension
            # For example: results_llm_01_DeepSeek-V3-0324.yaml
            # will extract the model "DeepSeek-V3-0324"

            for file in filtered_files:
                # Extract the model name using regex
                match = re.search(r"_(.+)\.yaml$", file)
                if match:
                    model_names.append(match.group(1))

    model_names = list(set(model_names))
    
    for model in model_names:

        agg_file_name = f"{results_path}/result_agg_{model}.yaml"
        q_agg = Questions(yaml_file=agg_file_name)

        # Filter the files to only the ones that contain the model name
        model_files = [f for f in filtered_files if f.endswith(model + ".yaml")]

        for f in model_files:

            questions = Questions(yaml_file=f)
            all_questions = questions.get_all_questions()

            iteration = "00"

            match = re.search(r"results_llm_(\d{2})_", f)
            if match:
                iteration = match.group(1)            

            # Add the questions to the q_agg object
            for question in all_questions:
                question['iteration'] = iteration
                q_agg.add_question(question)

        
        q_agg.save_questions(yaml_file=agg_file_name)
        print(f"Consolidated file {agg_file_name} generated.")
        

def consolidate_files_by_model(results_path: str, file_name_prefix: str):
    """
    Consolidate the files generated by the LLM into a single file.
    :param results_path: Path to the results folder.
    :param file_name_prefix: Prefix of the files to consolidate.
    :return: None
    """
    # Search for the generated files

    filtered_files = []

    for file in os.listdir(results_path):
        file_path = os.path.join(results_path, file)
        if os.path.isfile(file_path) and file.startswith(file_name_prefix) and file.endswith(".yaml"):
            filtered_files.append(file_path)

    # Extract the model as the last part of the file name and before the ".yaml" extension
    # For example: results_agg_DeepSeek-V3-0324.yaml
    # will extract the model "DeepSeek-V3-0324"

    agg_file_name = f"{results_path}/results_ALL_agg.yaml"
    q_agg = Questions(yaml_file=agg_file_name)

    for file in filtered_files:
        # Extract the model name using regex
        model_name = ""
        match = re.search(r"results_agg_(.+)\.yaml$", file)
        if match:
            model_name = match.group(1)

        questions = Questions(yaml_file=file)
        all_questions = questions.get_all_questions()

        # Add the model_name to the q_agg object
        for question in all_questions:
            question['model_name'] = model_name
            q_agg.add_question(question)

    q_agg.save_questions(yaml_file=agg_file_name)
    print(f"Consolidated file {agg_file_name} generated.")


def consolidate_csv_files(results_path: str, file_name_prefix: str):
    """
    Consolidate the CSV files generated by the LLM into a single file.
    :param results_path: Path to the results folder.
    :param file_name_prefix: Prefix of the files to consolidate.
    :return: None
    """
    # Search for the generated files

    filtered_files = []

    for file in os.listdir(results_path):
        file_path = os.path.join(results_path, file)
        if os.path.isfile(file_path) and file.startswith(file_name_prefix) and file.endswith(".csv"):
            filtered_files.append(file_path)

    # Create a consolidated CSV file
    consolidated_file_name = f"{results_path}/AGG_{file_name_prefix}.csv"
    header_written = False
    with open(consolidated_file_name, "w", newline="", encoding="utf-8") as outfile:
        writer = csv.writer(outfile)
        for file in filtered_files:
            with open(file, "r", newline="", encoding="utf-8") as infile:
                reader = csv.reader(infile)
                for i, row in enumerate(reader):
                    if i == 0:
                        if not header_written:
                            writer.writerow(row)
                            header_written = True
                        # Skip header for subsequent files
                        continue
                    writer.writerow(row)

    print(f"Consolidated CSV file {consolidated_file_name} generated.")


def performance_report(results_path: str, file_name_prefix: str):
    """
    Load all the CSV files in the results_path folder that start with the file_name_prefix.
    Calculate the performance of each model based on the number of rows, columns, time taken to execute the SQL query, and token costs.
    :param results_path: Path to the results folder.
    :param file_name_prefix: Prefix of the files to consolidate.
    :return: None
    """
    # Search for the generated files

    filtered_files = []

    for file in os.listdir(results_path):
        file_path = os.path.join(results_path, file)
        if os.path.isfile(file_path) and file.startswith(file_name_prefix) and file.endswith(".csv"):
            filtered_files.append(file_path)

    if not filtered_files:
        print("No files found with the specified prefix.")
        return

    # Read all files into dataframes
    dataframes = []
    for file in filtered_files:
        try:
            df = pd.read_csv(file, sep="\t", encoding="utf-8")
            dataframes.append(df)
        except Exception as e:
            print(f"Error reading {file}: {e}")

    if not dataframes:
        print("No files could be loaded correctly.")
        return

    # Concatenate all dataframes
    all_data = pd.concat(dataframes, ignore_index=True)

    all_data.rename(columns={
        'Model': 'model',
        'Question': 'question',
        'Rows': 'rows',
        'Columns': 'columns',
        'SQL_time': 'sql_time',
        'LLM_time': 'llm_time',
        'Total_tokens': 'total_tokens',
        'Prompt_tokens': 'prompt_tokens',
        'Completion_tokens': 'completion_tokens',
        'Percent_rows_equality': 'percent_rows_equality',
        'Percent_columns_equality': 'percent_columns_equality',
        'Percent_source_rows_equality': 'percent_source_rows_equality',
        'Percent_llm_rows_equality': 'percent_llm_rows_equality',
        'Cost_input_tokens_EUR': 'cost_input_tokens_EUR',
        'Cost_output_tokens_EUR': 'cost_output_tokens_EUR'
    }, inplace=True)    

    # Validate that the required columns exist
    required_columns = ['model', 'question', 'rows', 'columns', 'sql_time', 'llm_time',
                        'total_tokens', 'prompt_tokens', 'completion_tokens',
                        'percent_rows_equality', 'percent_columns_equality', 'percent_source_rows_equality',
                        'percent_llm_rows_equality', 'cost_input_tokens_EUR', 'cost_output_tokens_EUR']

    missing_columns = [col for col in required_columns if col not in all_data.columns]
    if missing_columns:
        print(f"The following required columns are missing in the data: {missing_columns}")
        return

    # Calculate the total token cost
    all_data['total_cost_tokens_EUR'] = all_data['cost_input_tokens_EUR'] + all_data['cost_output_tokens_EUR']

    # Standard aggregations
    agg = all_data.groupby('model').agg(
        queries_executed=('question', 'count'),
        mean_sql_time=('sql_time', 'mean'),
        mean_llm_time=('llm_time', 'mean'),
        stdev_llm_time=('llm_time', 'std'),
        mean_tokens=('total_tokens', 'mean'),
        mean_source_rows_equality=('percent_source_rows_equality', 'mean'),
        mean_llm_rows_equality=('percent_llm_rows_equality', 'mean'),
#        mean_rows_equality=('percent_rows_equality', 'mean'),
#        mean_columns_equality=('percent_columns_equality', 'mean'),
        mean_cost_EUR=('total_cost_tokens_EUR', 'mean')
    ).reset_index()

    cols_to_round = [
        'mean_sql_time', 'mean_llm_time', 'stdev_llm_time', 'mean_tokens',
        'mean_source_rows_equality', 'mean_llm_rows_equality', #'mean_rows_equality', 'mean_columns_equality'
    ]
    agg[cols_to_round] = agg[cols_to_round].round(2)
    agg['mean_cost_EUR'] = agg['mean_cost_EUR'].round(6)

    print("\nPerformance Report per model:\n")
    print(tabulate.tabulate(agg, headers='keys', tablefmt='pipe', showindex=False))

    agg_queries = all_data.groupby(['question']).agg(
        mean_llm_time=('llm_time', 'mean'),
        stdev_llm_time=('llm_time', 'std'),
        mean_source_rows_equality=('percent_source_rows_equality', 'mean'),
        mean_llm_rows_equality=('percent_llm_rows_equality', 'mean'),
        mean_rows_equality=('percent_rows_equality', 'mean'),
        mean_columns_equality=('percent_columns_equality', 'mean')
    ).reset_index()

    cols_to_round = [
        'mean_llm_time', 'stdev_llm_time',
        'mean_source_rows_equality', 'mean_llm_rows_equality', 'mean_rows_equality', 'mean_columns_equality'
    ]
    agg_queries[cols_to_round] = agg_queries[cols_to_round].round(2)

    print("\nPerformance Report per query:\n")

    print(tabulate.tabulate(agg_queries, headers='keys', tablefmt='pipe', showindex=False))

    # Print the best model names based on the average LLM time
    best_models = agg.sort_values(by='mean_llm_time')# .head(6)
    print(f"\nBest models based on average LLM time:\n\n")
    print(tabulate.tabulate(best_models[['model', 'mean_llm_time']], headers='keys', tablefmt='pipe', showindex=False))

    # Sort by mean token cost
    best_cost_models = agg.sort_values(by='mean_cost_EUR')# .head(6)
    print(f"\nBest models based on mean token cost:\n")
    print(tabulate.tabulate(best_cost_models[['model', 'mean_cost_EUR']], headers='keys', tablefmt='pipe', showindex=False))

    # Print the best model names based on the average data_rows_equality
    best_data_rows_equality = agg.sort_values(by='mean_source_rows_equality', ascending=False)#.head(6)
    print(f"\nBest models based on average data rows equality:\n\n")
    print(tabulate.tabulate(best_data_rows_equality[['model', 'mean_source_rows_equality']], headers='keys', tablefmt='pipe', showindex=False))

    # add a column to the agg dataframe that represents the ranking of the model based in the total cost EUR: name it rank_price
    agg['rank_quality'] = agg['mean_source_rows_equality'].rank(method='min', ascending=False).astype(int)
    agg['rank_price'] = agg['mean_cost_EUR'].rank(method='min', ascending=True).astype(int)
    agg['rank_time'] = agg['mean_llm_time'].rank(method='min', ascending=True).astype(int)

    cols_rank = ['rank_quality', 'rank_time', 'rank_price']

    # Convertir rankings mayores que 8 en ''
    for col in cols_rank:
        agg[col] = agg[col].apply(lambda x: str(x) if x <= 8 and x > 0 else '')

    # añade a cada columna de rank_quality un guion - y en valor entre () de mean_source_rows_equality cuando la columna no esté vacia
    agg['rank_quality'] = agg.apply(
        lambda row: f"{row['rank_quality']} ({row['mean_source_rows_equality']})" if row['rank_quality'] != '' else '',
        axis=1
    )
    agg['rank_time'] = agg.apply(
        lambda row: f"{row['rank_time']} ({row['mean_llm_time']})" if row['rank_time'] != '' else '',
        axis=1
    )
    agg['rank_price'] = agg.apply(
        lambda row: f"{row['rank_price']} ({row['mean_cost_EUR']})" if row['rank_price'] != '' else '',
        axis=1
    )

    # Paso 1: Convertir valores a números temporales para ordenar
    for col in cols_rank:
        agg[f"_sort_{col}"] = agg[col].apply(lambda x: int(str(x).split()[0]) if x != '' else np.inf)

    # Paso 2: Ordenar usando las columnas auxiliares
    agg = agg.sort_values(
        by=[f"_sort_{col}" for col in cols_rank],
        ascending=True
    ).reset_index(drop=True)

    # Paso 3: Eliminar columnas auxiliares
    agg.drop(columns=[f"_sort_{col}" for col in cols_rank], inplace=True)

    print("\n\nRanking of the models based on the total cost, LLM time and source rows equality:\n")
    print(tabulate.tabulate(agg[['model', 'rank_quality', 'rank_time', 'rank_price']], headers='keys', tablefmt='pipe', showindex=False))

    print(f"\n\n")

